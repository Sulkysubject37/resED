\section{Experimental Design}

Our experimental campaign is designed to test the central hypothesis that reliability can be governed at the representation level across diverse model families and data domains. We systematically dismantle the assumption that model components are intrinsically robust, and instead verify that the system-level governance layer can provide the necessary safety guarantees. We structure our validation around four primary scientific questions.

\subsection{Objective I: Observability of Representation Failure}
\textbf{Hypothesis:} System-level failures such as drift and shock manifest as monotonic deviations in RLCS metrics before they cause observable output errors.
\textbf{Motivation:} If failure modes are silent in the latent space (i.e., the encoder "hides" the error), governance is impossible. We must prove that statistical distance is a valid proxy for semantic corruption.
\textbf{Setup:} We utilized the \texttt{resED} pipeline with synthetic inputs to ensure total control over the latent manifold. We injected deterministic perturbations:
\begin{itemize}
    \item \textbf{Gradual Drift}: Linear shift of the latent mean over time, simulating environment change.
    \item \textbf{Sudden Shock}: High-magnitude noise injection (\(\sigma=10.0\)) at a single time step, simulating sensor glitch or adversarial input.
\end{itemize}
\textbf{Metrics:} We monitored the ResLik (\(D\)) and TCS (\(T\)) response curves to establish detection sensitivity. A successful outcome is a monotonic rise in scores crossing the safety threshold \(\tau\).

\subsection{Objective II: Governance Efficacy and Suppression}
\textbf{Hypothesis:} The governed system will suppress hallucinations that an ungoverned model would otherwise generate.
\textbf{Motivation:} Detection is useless without intervention. We must demonstrate that the system can actively prevent the propagation of corrupt data to the user.
\textbf{Setup:} We performed a comparative run between:
\begin{itemize}
    \item \textbf{resED OFF}: RLCS is bypassed; the decoder executes on corrupted latents.
    \item \textbf{resED ON}: RLCS is active; the decoder is gated by control signals.
\end{itemize}
\textbf{Metrics:} We measured the output norm \(\left\|y\right\|\) and the transition timing of the \texttt{ABSTAIN} signal. We expect the governed system to yield \(\left\|y\right\| \to 0\) (suppression) during the shock, shielding the downstream consumer.

\subsection{Objective III: High-Dimensional Domain Transfer}
\textbf{Hypothesis:} Distance-based thresholds calibrated on low-dimensional data will fail on high-dimensional biological embeddings due to the curse of dimensionality (\(\mathbb{E}[\|z\|] \propto \sqrt{d}\)), requiring formal calibration.
\textbf{Motivation:} A scalable governance architecture must work across domains without manual "magic number" tuning. We test if our calibration layer enables this universality.
\textbf{Setup:}
\begin{itemize}
    \item \textbf{Vision Baseline}: CIFAR-10 embeddings extracted via a pre-trained ResNet-50.
    \item \textbf{Biological Benchmark}: 128-dimensional gene embeddings from Bioteque (\texttt{GEN-\_dph-GEN} metapath).
\end{itemize}
\textbf{Comparison:} We evaluated clean acceptance rates under uncalibrated versus reference-conditioned calibration.
\textbf{Metrics:} Acceptance rate (PROCEED) on clean data vs. rejection rate (ABSTAIN) on noise (\(\sigma=0.6\)). We expect Condition A to reject clean data (False Positive) and Condition B to accept it while still rejecting noise.

\subsection{Objective IV: Component Sensitivity and Universality}
\textbf{Hypothesis:} Individual modules lack intrinsic safety mechanisms and will propagate or amplify errors if not governed.
\textbf{Motivation:} To justify the cost of the RLCS layer, we must prove that the components themselves (Encoders, Transformers) are not "safe by default."
\textbf{Setup:} We isolated each component to define its empirical failure envelope:
\begin{itemize}
    \item \textbf{resENC Stability}: Measured L2 distortion under input noise \(\sigma \in [0.01, 0.3]\).
    \item \textbf{resTR Sensitivity}: Measured attention entropy collapse under token corruption (\(N \in \{1, 5\}\).
    \item \textbf{resDEC Volatility}: Quantified the sensitivity ratio (\(\Delta y / \Delta z\)).
\end{itemize}
\textbf{Metrics:} We derived min/max envelopes for each component's response to stress.