\section{Results}

Our findings demonstrate that representation-level observability provides a reliable substrate for system-level governance. We present evidence across synthetic, vision, and biological domains.

\subsection{Detection and Observability}
The RLCS sensors successfully convert latent perturbations into observable signals. As shown in \textbf{Figure \ref{fig:stress_obs}}, both ResLik and TCS sensors track latent drift with high monotonicity. The ResLik score provides an early-warning signal, crossing the $\tau_D=3.0$ safety threshold well before the representation is completely corrupted. This monotonicity is critical; it ensures that there are no "blind spots" where error increases but the signal remains flat.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/figure1_stress_observability.png}
    \caption{RLCS Sensor Observability. ResLik and TCS scores track latent drift, triggering ABSTAIN and DEFER signals respectively.}
    \label{fig:stress_obs}
\end{figure*}

\subsection{Efficacy of Gated Decoding}
\textbf{Figure \ref{fig:on_off}} contrasts the behavior of the governed (\textit{resED ON}) and ungoverned (\textit{resED OFF}) systems. To ensure statistical rigor, we performed an aggregate simulation over $N=1,000$ independent shock events. 
\begin{itemize}
    \item The \textbf{Ungoverned System (Grey)} consistently produced high-variance outputs during shock (mean norm $\approx 1.5$), demonstrating the danger of "blind" decoding on corrupted latents.
    \item The \textbf{Governed System (Green)} successfully transitioned to \texttt{ABSTAIN} in 100% of shock trials, resulting in total output suppression (mean norm $\to 0$).
\end{itemize}
This result confirms that reliability is a deterministic function of the control surface architecture, not an anecdotal observation. The system successfully prioritized silence over speculative generation across the entire experimental population.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/figure2_resed_on_off.png}
    \caption{System Response. Governance prevents hallucination by suppressing output during shock events.}
    \label{fig:on_off}
\end{figure*}

\subsection{Baseline Comparison}
To contextualize the performance of RLCS, we benchmarked it against a standard Mahalanobis distance detector using full covariance estimation (Table \ref{tab:auroc}). Under high-magnitude perturbations (Noise $\sigma \ge 0.1$, Shock, Drift), RLCS achieved parity with the baseline (AUROC $\approx 1.0$), demonstrating that scalar governance is sufficient for detecting catastrophic failures. However, under subtle noise conditions ($\sigma=0.05$), the full-covariance Mahalanobis detector outperformed RLCS (AUROC 0.98 vs 0.79), highlighting the trade-off between computational efficiency and sensitivity to fine-grained correlations.

\begin{table}[t!]
\centering
\caption{Empirical AUROC Comparison (CIFAR-10 Embeddings)}
\label{tab:auroc}
\begin{tabular}{lccc}
\toprule
\textbf{Perturbation} & \textbf{Mahalanobis} & \textbf{RLCS (Ours)} & \textbf{Difference} \\
\midrule
Noise ($\\sigma=0.05$) & 0.980 & 0.791 & -0.189 \\
Noise ($\\sigma=0.10$) & 1.000 & 0.998 & -0.002 \\
Shock ($1.5\times$) & 0.958 & 0.998 & +0.040 \\
Shock ($5.0\times$) & 1.000 & 1.000 & 0.000 \\
Drift ($+2.0$) & 1.000 & 1.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dimensionality Scaling and Calibration}
A key finding is the impact of dimensionality on uncalibrated distance metrics. As shown in \textbf{Table \ref{tab:gov_outcomes}}, raw distance thresholds tuned for synthetic data (64D) failed catastrophically on high-dimensional benchmarks, rejecting 100% of valid data for Vision (2048D) and Biology (128D) due to the curse of dimensionality ($\\mathbb{E}[\|z\|] \propto \sqrt{d}$). 

The Reference-Conditioned Calibration Layer effectively neutralized this scaling factor. By mapping raw distances to a reference-relative Z-score, the clean acceptance rate was restored to >99% across all domains, without manual threshold retuning.

\begin{table*}[t!]
\centering
\caption{Governance Outcomes Across Domains (Acceptance Rate \%)}
\label{tab:gov_outcomes}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Synthetic (64D)} & \textbf{Vision (2048D)} & \textbf{Biology (128D)} \\
\midrule
Clean (Uncalibrated) & 99.8\% & 0.0\%* & 0.0\%* \\
Clean (Calibrated)   & 99.8\% & 99.7\% & 99.6\% \\
Noise ($\\sigma=0.6$) & 0.0\%  & 0.0\%  & 0.0\% \\
Shock (5\%)          & 95.0\% & 95.0\% & 95.0\% \\
\bottomrule
\end{tabular}
\par\smallskip
{\\footnotesize *Rejection due to dimensionality scaling mismatch.}
\end{table*}

\begin{figure*}[t!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/figure4a_bioteque_calibrated_sensor_response.pdf}
        \\ (a) Calibrated Sensor Response
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/figure4b_bioteque_calibrated_control_distribution.pdf}
        \\ (b) Control Distribution
    \end{minipage}
    \caption{Biological Validation. Calibration restores utility on high-dimensional data without compromising safety.}
    \label{fig:bio_calib}
\end{figure*}

\subsection{Distributional Shift and Circularity}
To test the "Reference Dependency" limitation, we performed a circularity test by clustering the CIFAR-10 embedding space into two modes ($C_0, C_1$) and using $C_0$ as the reference to judge $C_1$. The system mostly responded with \texttt{DOWNWEIGHT} or \texttt{DEFER} rather than outright \texttt{ABSTAIN} (Rejection Rate $< 1\%$), indicating that while the valid shifted population was detected as "atypical" (Warning Zone), it was not rejected as "impossible" (Critical Zone). This confirms the graded response capability.

Figure \ref{fig:histograms} visualizes this behavior: Shifted Valid data (Orange) overlaps the Reference tail (Green) but is distinct, whereas OOD Noise (Red) is completely separated.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure_d_score_histograms.pdf}
    \caption{Distribution of Risk Scores. Shifted valid data triggers warning thresholds, while noise triggers rejection.}
    \label{fig:histograms}
\end{figure*}

\subsection{Empirical Failure Envelopes}
Component stress testing revealed the intrinsic limits of the modules. As summarized in \textbf{Table \ref{tab:failure_envelopes}}, all components lack internal stability mechanisms. The encoder amplifies input noise linearly. The transformer, often assumed to be robust, suffers from a catastrophic "attention collapse" under heavy corruption.

\begin{table}[t!]
\centering
\caption{Summary of Component Failure Envelopes}
\label{tab:failure_envelopes}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Observed Failure Mode} & \textbf{Impact on Output} \\
\midrule
resENC & Variance Inflation & Radial Drift \\
resTR  & Attention Collapse & Noise Fixation \\
resDEC & Linear Error Prop. & Hallucination \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure_component_resenc_stability.pdf}
    \caption{Encoder Stability. Latent distortion scales linearly with input noise.}
    \label{fig:enc_stab}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure_component_restr_sensitivity.pdf}
    \caption{Transformer Sensitivity. Attention entropy collapses under heavy corruption.}
    \label{fig:tr_sens}
\end{figure*}