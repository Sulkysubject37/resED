\section{Limitations}

While the resED framework offers a robust mechanism for system-level governance, it is subject to specific operational boundaries.

\subsection{Architectural Blind Spots}
Our cross-architecture validation identified a critical limitation in Transformer-based encoders utilizing Layer Normalization. Because LayerNorm projects latent vectors onto a fixed hypersphere, pure magnitude-based perturbations (Shock) are effectively normalized away before they reach the RLCS sensors. While the system remains sensitive to directional shifts (Drift), this "Normalization Blindness" means that for certain architectures, the RLCS must be augmented with pre-normalization sensors to maintain full observability.

\subsection{Dependency on Failure Manifestation}
The RLCS relies on the premise that semantic failure manifests as statistical anomaly in the latent space. While this holds for the stochastic and distributional failures tested (Noise, Drift, Shock), it may not hold for sophisticated adversarial attacks designed to minimize statistical distance while maximizing semantic error. The system guards the manifold, not the semantic meaning; a statistically "typical" representation of nonsense will theoretically result in a \texttt{PROCEED} decision, although such a vector is difficult to produce without violating the manifold constraints.

\subsection{Reference Dependency}
The governance logic is strictly conditioned on the reference population $\mathcal{P}_{ref}$. If the operational environment undergoes valid concept drift (e.g., a new biological condition emerges), the system will correctly flag it as anomalous. Distinguishing between "invalid drift" (failure) and "valid drift" (discovery) requires an external update to the reference set. The system is conservative by design; it does not "learn" to accept new data online.
