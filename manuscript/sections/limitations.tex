\section{Limitations}

While the resED framework offers a robust mechanism for system-level governance, it is subject to specific operational boundaries.

\subsection{Architectural Blind Spots: Normalization Blindness}
We explicitly retain and highlight a critical limitation: "Normalization Blindness." Transformer architectures utilizing Layer Normalization project latent vectors onto a fixed hypersphere, effectively erasing magnitude-based error signals. This means LayerNorm suppresses magnitude-based failure signals before they reach the RLCS sensors. Consequently, our claims regarding failure interception are scoped to non-normalized or pre-norm representations. For normalized architectures, the RLCS must be augmented with pre-normalization sensors to maintain full observability. Future work may explore pre-normalization residual monitoring or norm-preserving probes for normalized architectures.

\subsection{Sensitivity to Optimized Perturbations}
We explicitly acknowledge that RLCS does not claim adversarial completeness. The distance-based sensors (ResLik) rely on the assumption that failure modes manifest as statistical anomalies in the latent geometry. While effective against stochastic noise and distributional drift, these sensors can theoretically be evaded by optimized adversarial perturbations designed to minimize Mahalanobis distance while maximizing semantic error. We treat adversarial robustness as a distinct, orthogonal challenge; resED provides a baseline of "natural safety" but should be paired with adversarial training for hostile environments.

\subsection{Dependency on Failure Manifestation}
The RLCS relies on the premise that semantic failure manifests as statistical anomaly in the latent space. The system guards the manifold, not the semantic meaning; a statistically "typical" representation of nonsense will theoretically result in a \texttt{PROCEED} decision, although such a vector is difficult to produce without violating the manifold constraints.

\subsection{Reference Dependency}
The governance logic is strictly conditioned on the reference population $\mathcal{P}_{ref}$. As demonstrated by our circularity test, the system cannot distinguish between "valid new data" (e.g., a new class) and "invalid drift" without an external update to the reference set. The system is conservative by design; it treats all deviations from the reference manifold as potential risks.
