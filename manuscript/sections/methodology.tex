\section{Methodology}

The core innovation of the resED framework is the Representation-Level Control Surface (RLCS), a deterministic mechanism for governing the behavior of opaque generative models. This section details the mathematical foundations of the RLCS sensors, the calibration logic, and the control policy.

\subsection{Representation-Level Control Surface (RLCS)}
The RLCS operates on the principle of "Trust but Verify." It does not attempt to interpret the semantic content of a latent vector but instead evaluates its statistical consistency with a known "trust manifold." This manifold is defined by the empirical distribution of valid representations from a reference population $\mathcal{P}_{ref}$.

\subsubsection{Population Consistency (ResLik)}
The ResLik sensor measures the Mahalanobis-like distance of a new representation $z$ from the historical centroid of the reference population. To ensure scalability, we approximate this using a standardized Euclidean distance:
\begin{equation}
D(z) = \frac{\|z - \mu\|_2}{\sigma + \epsilon}
\end{equation}
where $\mu = \mathbb{E}[z]$ and $\sigma = \sqrt{\mathbb{V}[z]}$ are parameters estimated from $\mathcal{P}_{ref}$. A high $D(z)$ indicates a statistical anomaly, or "Stranger," suggesting the input lies outside the valid operational envelope of the encoder. Unlike classifier logits, which can be overconfident far from the decision boundary, this distance metric is monotonic and unbounded, preserving the magnitude of the anomaly.

\subsubsection{Temporal Consistency Sensor (TCS)}
For sequential data, the system enforces trajectory smoothness. The TCS monitors the rate of change in the latent space:
\begin{equation}
T(z_t, z_{t-1}) = \exp(-\|z_t - z_{t-1}\|_2)
\end{equation}
A sudden collapse in $T$ indicates a "Jitter" failureâ€”an unphysical discontinuity in the latent trajectory that often precedes semantic collapse. This sensor acts as a temporal low-pass filter for trust.

\subsection{Reference-Conditioned Calibration}
A critical requirement for system-level reliability is the ability to define thresholds that generalize across domains. In high-dimensional spaces, the expected Euclidean distance scales with $\sqrt{d}$, making raw distance thresholds domain-specific and brittle.

To resolve this, we introduce a \textbf{Reference-Conditioned Calibration Layer}. This layer transforms raw diagnostic scores $D_{raw}$ into a universal risk coordinate (Z-score) using empirical quantile matching against the reference set:
\begin{equation}
\hat{D}(z) = \Phi^{-1}(P(D \le D_{raw} | \mathcal{P}_{ref}))
\end{equation}
where $\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. By mapping the empirical distribution of distances to a standard normal, we ensure that a threshold of $\hat{D} > 3.0$ consistently represents a "3-sigma" statistical rarity, regardless of whether the embedding space has 10 dimensions or 1024. This allows the same governance policy to be applied to both CIFAR-10 (vision) and Bioteque (biology) without manual retuning.

\subsection{Control Policy}
The signals from ResLik and TCS are aggregated to form a discrete control signal $\pi$. The policy enforces a conservative "circuit-breaker" logic:
\begin{itemize}
    \item If $\hat{D}(z) > \tau_{critical}$: Signal \texttt{ABSTAIN}.
    \item If $\tau_{warning} < \hat{D}(z) \le \tau_{critical}$: Signal \texttt{DOWNWEIGHT}.
    \item Otherwise: Signal \texttt{PROCEED}.
\end{itemize}
This deterministic policy ensures that the system's response to uncertainty is predictable and verifiable.
