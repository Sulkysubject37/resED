\section{Methodology}

The core innovation of the resED framework is the Representation-Level Control Surface (RLCS), a deterministic mechanism for governing the behavior of opaque generative models. This section details the mathematical foundations of the RLCS sensors, the calibration logic, and the control policy.

\subsection{Non-Claims and Scope}
\begin{center}
\fbox{\begin{minipage}{0.95\columnwidth}
\textbf{Non-Claims and Scope}
\begin{itemize}
    \item \textbf{No Correctness Guarantee}: resED does not guarantee correctness, optimality, or adversarial robustness.
    \item \textbf{Operational Reliability}: The system guarantees failure observability, containment, and suppression, not prediction accuracy.
    \item \textbf{Reliability Definition}: Reliability is defined operationally as controlled failure behavior (e.g., safe shutdown), not as the correctness of the generated output.
\end{itemize}
\end{minipage}}
\end{center}

\subsection{Representation-Level Control Surface (RLCS)}
The RLCS operates on the principle of "Trust but Verify." It does not attempt to interpret the semantic content of a latent vector but instead evaluates its statistical consistency with a known "trust manifold." This manifold is defined by the empirical distribution of valid representations from a reference population $\mathcal{P}_{ref}$.

\textbf{Definition (Circuit Breaker):} A deterministic mechanism that suppresses unsafe outputs to a zero-information state (silence) rather than providing low-confidence estimates.

\subsubsection{Population Consistency (ResLik)}
The ResLik sensor serves as an isotropic, distance-based monitor. It measures the deviation of a new representation $z$ from the historical centroid of the reference population. To ensure scalability and suitability for real-time governance, we use a simplified Euclidean metric:
\begin{equation}
D(z) = \frac{\|z - \mu\|_2}{\sigma + \epsilon}
\end{equation}
where $\mu = \mathbb{E}[z]$ and $\sigma = \sqrt{\mathbb{V}[z]}$ are parameters estimated from $\mathcal{P}_{ref}$. 

\textbf{Justification:} While ResLik does not claim to outperform full Mahalanobis distance as a density estimator, its selection is justified by:
\begin{itemize}
    \item \textbf{Determinism}: It provides a stable, repeatable signal.
    \item \textbf{Linear Complexity}: Its $\mathcal{O}(d)$ complexity scales to high dimensions where full covariance estimation ($\mathcal{O}(d^3)$) is prohibitive.
    \item \textbf{Governance Suitability}: It provides a monotonic signal sufficient for "pass/fail" gating logic.
\end{itemize}

A high $D(z)$ indicates a statistical anomaly, or "Stranger," suggesting the input lies outside the valid operational envelope of the encoder. Unlike classifier logits, which can be overconfident far from the decision boundary, this distance metric is monotonic and unbounded, preserving the magnitude of the anomaly.

\subsubsection{Temporal Consistency Sensor (TCS)}
For sequential data, the system enforces trajectory smoothness. The TCS monitors the rate of change in the latent space:
\begin{equation}
T(z_t, z_{t-1}) = \exp(-\|z_t - z_{t-1}\|_2)
\end{equation}
A sudden collapse in $T$ indicates a "Jitter" failureâ€”an unphysical discontinuity in the latent trajectory that often precedes semantic collapse.

\subsubsection{Multi-View Agreement Sensor (MVA)}
To further robustify the governance against subtle corruptions, we introduce the Multi-View Agreement sensor. This sensor enforces the invariant that valid representations should be invariant to semantics-preserving transformations of the input. For an input $x$ and a set of augmentations $\mathcal{T}$, MVA measures the divergence:
\begin{equation}
A(z) = \frac{1}{|\mathcal{T}|} \sum_{t \in \mathcal{T}} \|z - E(t(x))\|_2
\end{equation}
High divergence suggests that the encoder is unstable or that the input lies near a decision boundary cliff, warranting caution (e.g., \texttt{DOWNWEIGHT}).

\subsection{Reference-Conditioned Calibration}
A critical requirement for system-level reliability is the ability to define thresholds that generalize across domains. In high-dimensional spaces, the expected Euclidean distance scales with $\sqrt{d}$, making raw distance thresholds domain-specific and brittle.

To resolve this, we introduce a \textbf{Reference-Conditioned Calibration Layer}. This layer transforms raw diagnostic scores $D_{raw}$ into a universal risk coordinate (Z-score) using empirical quantile matching against the reference set:
\begin{equation}
\hat{D}(z) = \Phi^{-1}(P(D \le D_{raw} | \mathcal{P}_{ref}))
\end{equation}
where $\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. 

\textbf{Assumption Clarification:} This process does \textit{not} assume that the underlying data or raw distances follow a Gaussian distribution. Instead, it employs non-parametric quantile mapping to \textit{force} the calibrated risk scores of the reference population to follow a standard normal distribution $\mathcal{N}(0, 1)$. This normalization allows us to define universal thresholds (e.g., $\hat{D} > 3.0$ implies a "3-sigma" rarity relative to the reference) regardless of the intrinsic geometry or modality of the embedding space.

\subsection{Control Policy}
The signals from ResLik, TCS, and MVA are aggregated to form a discrete control signal $\pi$. The policy enforces a conservative "circuit-breaker" logic:
\begin{itemize}
    \item If $\hat{D}(z) > \tau_{critical}$: Signal \texttt{ABSTAIN}.
    \item If $\tau_{warning} < \hat{D}(z) \le \tau_{critical}$: Signal \texttt{DOWNWEIGHT}.
    \item Otherwise: Signal \texttt{PROCEED}.
\end{itemize}
This deterministic policy ensures that the system's response to uncertainty is predictable and verifiable.
